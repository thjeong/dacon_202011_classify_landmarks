{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XDnap1jLv8so"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as pth\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ffY3gSLAvvSs"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = 'InceptionV3-for-upload'\n",
    "my_model_base = keras.applications.inception_v3\n",
    "my_model = my_model_base.InceptionV3\n",
    "\n",
    "config = {\n",
    "    'is_zscore':True,\n",
    "    \n",
    "    # 'input_shape': (540, 960, 3),\n",
    "    'aug': {\n",
    "        #'resize': (270, 480),\n",
    "        'resize': (297, 528),\n",
    "    },\n",
    "    # 'input_shape': (224, 360, 3),\n",
    "    #'input_shape': (270, 480, 3),\n",
    "    'input_shape': (270, 480, 3),\n",
    "\n",
    "    'output_activation': 'softmax',\n",
    "    'num_class': 1049,\n",
    "    'output_size': 1049,\n",
    "    \n",
    "    'conv':{\n",
    "        'conv_num': (0), # (3,5,3),\n",
    "        'base_channel': 0, # 4,\n",
    "        'kernel_size': 0, # 3,\n",
    "        'padding':'same',\n",
    "        'stride':'X'\n",
    "    },\n",
    "    'pool':{\n",
    "        'type':'X',\n",
    "        'size':'X',\n",
    "        'stride':'X',\n",
    "        'padding':'same'\n",
    "    },\n",
    "    'fc':{\n",
    "        'fc_num': 0,\n",
    "     },\n",
    "    \n",
    "    'activation':'relu',\n",
    "    \n",
    "    'between_type': 'avg',\n",
    "    \n",
    "    'is_batchnorm': True,\n",
    "    'is_dropout': False,\n",
    "    'dropout_rate': 0.5,\n",
    "    \n",
    "    'add_dense':True,\n",
    "    'dense_size': 1024,\n",
    "    \n",
    "    'batch_size': 64, #64,\n",
    "    'buffer_size': 256, #256,\n",
    "    'loss': 'CategoricalCrossentropy',\n",
    "    \n",
    "    'num_epoch': 10000,\n",
    "    'learning_rate': 1e-5, #1e-3,\n",
    "    \n",
    "    'random_state': 7777\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "abOw4s0jv6JI"
   },
   "outputs": [],
   "source": [
    "image_feature_description = {\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'randmark_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    # 'id': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = target_record['randmark_id']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "def resize_and_crop_func(image, label):\n",
    "    result_image = tf.image.resize(image, config['aug']['resize'])\n",
    "    result_image = tf.image.random_crop(image, size=config['input_shape'], seed=7777)\n",
    "    return result_image, label\n",
    "\n",
    "def image_aug_func(image, label):\n",
    "    pass\n",
    "    return image, label\n",
    "\n",
    "def post_process_func(image, label):\n",
    "    # result_image = result_image / 255\n",
    "    result_image = my_model_base.preprocess_input(image)\n",
    "    onehot_label = tf.one_hot(label, depth=config['num_class'])\n",
    "    return result_image, onehot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j1UN3LYJzFgd"
   },
   "outputs": [],
   "source": [
    "data_base_path = pth.join('data', 'public')  \n",
    "os.makedirs(data_base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ks1l_51cNzLP"
   },
   "outputs": [],
   "source": [
    "category_csv_name = 'category.csv'\n",
    "category_json_name = 'category.json'\n",
    "submission_csv_name = 'sample_submisstion.csv'\n",
    "train_csv_name = 'train.csv'\n",
    "\n",
    "# train_zip_name = 'train.zip'\n",
    "train_tfrecord_name = 'all_train.tfrecords'\n",
    "train_tfrecord_path = pth.join(data_base_path, train_tfrecord_name)\n",
    "val_tfrecord_name = 'all_val.tfrecords'\n",
    "val_tfrecord_path = pth.join(data_base_path, val_tfrecord_name)\n",
    "# test_zip_name = 'test.zip'\n",
    "test_tfrecord_name = 'test.tfrecords'\n",
    "test_tfrecord_path = pth.join(data_base_path, test_tfrecord_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MaBBHyX0dMig"
   },
   "outputs": [],
   "source": [
    "train_csv_path = pth.join(data_base_path, train_csv_name)\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_dict = {k:v for k, v in train_df.values}\n",
    "\n",
    "submission_csv_path = pth.join(data_base_path, submission_csv_name)\n",
    "submission_df = pd.read_csv(submission_csv_path)\n",
    "# submission_df.head()\n",
    "\n",
    "category_csv_path = pth.join(data_base_path, category_csv_name)\n",
    "category_df = pd.read_csv(category_csv_path)\n",
    "category_dict = {k:v for k, v in category_df.values}\n",
    "# category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/public/all_train.tfrecords'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfrecord_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdng6pk8k0fH"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q9-4T5OMcy1R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GroupKFold, RepeatedStratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as pth\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from itertools import product, combinations\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "import datetime\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv3D, AveragePooling3D, MaxPooling3D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool3D, GlobalAvgPool3D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HBKtUQ9mnKMn"
   },
   "outputs": [],
   "source": [
    "conv_comb_list = []\n",
    "conv_comb_list += [(0,)]\n",
    "\n",
    "base_channel_list = [0]\n",
    "\n",
    "fc_list = [0] # 128, 0\n",
    "\n",
    "# between_type_list = [None, 'avg', 'max']\n",
    "between_type_list = ['avg']\n",
    "\n",
    "batch_size_list = [80]\n",
    "\n",
    "activation_list = ['relu']\n",
    "\n",
    "# len(conv_comb_list), conv_comb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAaKPD3cnKB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WrRPrv1aoOEA"
   },
   "outputs": [],
   "source": [
    "def build_cnn(config):\n",
    "    input_layer = Input(shape=config['input_shape'], name='input_layer')\n",
    "    pret_model = my_model(\n",
    "        input_tensor=input_layer, include_top=False, weights='imagenet', \n",
    "        input_shape=config['input_shape'], pooling=config['between_type'], \n",
    "        classes=config['output_size']\n",
    "    )\n",
    "\n",
    "    pret_model.trainable = False\n",
    "    \n",
    "    x = pret_model.output\n",
    "    \n",
    "    if config['between_type'] == None:\n",
    "        x = Flatten(name='flatten_layer')(x)\n",
    "        \n",
    "    if config['is_dropout']:\n",
    "        x = Dropout(config['dropout_rate'], name='output_dropout')(x)    \n",
    "\n",
    "    if config['add_dense']:\n",
    "        x = Dense(config['dense_size'], activation=config['activation'],\n",
    "                    name='dense_layer')(x)\n",
    "\n",
    "    x = Dense(config['output_size'], activation=config['output_activation'], \n",
    "          name='output_fc')(x)\n",
    "#     x = Activation(activation=config['output_activation'], name='output_activation')(x)\n",
    "    model = Model(inputs=input_layer, outputs=x, name='{}'.format(BASE_MODEL_NAME))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d5mZ06q3qAmN",
    "outputId": "edc1edc4-147a-43c3-e8c4-0ae69fa5e59d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_layer\n",
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_6\n",
      "24 batch_normalization_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_5\n",
      "33 batch_normalization_7\n",
      "34 batch_normalization_10\n",
      "35 batch_normalization_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n",
      "41 conv2d_15\n",
      "42 batch_normalization_15\n",
      "43 activation_15\n",
      "44 conv2d_13\n",
      "45 conv2d_16\n",
      "46 batch_normalization_13\n",
      "47 batch_normalization_16\n",
      "48 activation_13\n",
      "49 activation_16\n",
      "50 average_pooling2d_1\n",
      "51 conv2d_12\n",
      "52 conv2d_14\n",
      "53 conv2d_17\n",
      "54 conv2d_18\n",
      "55 batch_normalization_12\n",
      "56 batch_normalization_14\n",
      "57 batch_normalization_17\n",
      "58 batch_normalization_18\n",
      "59 activation_12\n",
      "60 activation_14\n",
      "61 activation_17\n",
      "62 activation_18\n",
      "63 mixed1\n",
      "64 conv2d_22\n",
      "65 batch_normalization_22\n",
      "66 activation_22\n",
      "67 conv2d_20\n",
      "68 conv2d_23\n",
      "69 batch_normalization_20\n",
      "70 batch_normalization_23\n",
      "71 activation_20\n",
      "72 activation_23\n",
      "73 average_pooling2d_2\n",
      "74 conv2d_19\n",
      "75 conv2d_21\n",
      "76 conv2d_24\n",
      "77 conv2d_25\n",
      "78 batch_normalization_19\n",
      "79 batch_normalization_21\n",
      "80 batch_normalization_24\n",
      "81 batch_normalization_25\n",
      "82 activation_19\n",
      "83 activation_21\n",
      "84 activation_24\n",
      "85 activation_25\n",
      "86 mixed2\n",
      "87 conv2d_27\n",
      "88 batch_normalization_27\n",
      "89 activation_27\n",
      "90 conv2d_28\n",
      "91 batch_normalization_28\n",
      "92 activation_28\n",
      "93 conv2d_26\n",
      "94 conv2d_29\n",
      "95 batch_normalization_26\n",
      "96 batch_normalization_29\n",
      "97 activation_26\n",
      "98 activation_29\n",
      "99 max_pooling2d_2\n",
      "100 mixed3\n",
      "101 conv2d_34\n",
      "102 batch_normalization_34\n",
      "103 activation_34\n",
      "104 conv2d_35\n",
      "105 batch_normalization_35\n",
      "106 activation_35\n",
      "107 conv2d_31\n",
      "108 conv2d_36\n",
      "109 batch_normalization_31\n",
      "110 batch_normalization_36\n",
      "111 activation_31\n",
      "112 activation_36\n",
      "113 conv2d_32\n",
      "114 conv2d_37\n",
      "115 batch_normalization_32\n",
      "116 batch_normalization_37\n",
      "117 activation_32\n",
      "118 activation_37\n",
      "119 average_pooling2d_3\n",
      "120 conv2d_30\n",
      "121 conv2d_33\n",
      "122 conv2d_38\n",
      "123 conv2d_39\n",
      "124 batch_normalization_30\n",
      "125 batch_normalization_33\n",
      "126 batch_normalization_38\n",
      "127 batch_normalization_39\n",
      "128 activation_30\n",
      "129 activation_33\n",
      "130 activation_38\n",
      "131 activation_39\n",
      "132 mixed4\n",
      "133 conv2d_44\n",
      "134 batch_normalization_44\n",
      "135 activation_44\n",
      "136 conv2d_45\n",
      "137 batch_normalization_45\n",
      "138 activation_45\n",
      "139 conv2d_41\n",
      "140 conv2d_46\n",
      "141 batch_normalization_41\n",
      "142 batch_normalization_46\n",
      "143 activation_41\n",
      "144 activation_46\n",
      "145 conv2d_42\n",
      "146 conv2d_47\n",
      "147 batch_normalization_42\n",
      "148 batch_normalization_47\n",
      "149 activation_42\n",
      "150 activation_47\n",
      "151 average_pooling2d_4\n",
      "152 conv2d_40\n",
      "153 conv2d_43\n",
      "154 conv2d_48\n",
      "155 conv2d_49\n",
      "156 batch_normalization_40\n",
      "157 batch_normalization_43\n",
      "158 batch_normalization_48\n",
      "159 batch_normalization_49\n",
      "160 activation_40\n",
      "161 activation_43\n",
      "162 activation_48\n",
      "163 activation_49\n",
      "164 mixed5\n",
      "165 conv2d_54\n",
      "166 batch_normalization_54\n",
      "167 activation_54\n",
      "168 conv2d_55\n",
      "169 batch_normalization_55\n",
      "170 activation_55\n",
      "171 conv2d_51\n",
      "172 conv2d_56\n",
      "173 batch_normalization_51\n",
      "174 batch_normalization_56\n",
      "175 activation_51\n",
      "176 activation_56\n",
      "177 conv2d_52\n",
      "178 conv2d_57\n",
      "179 batch_normalization_52\n",
      "180 batch_normalization_57\n",
      "181 activation_52\n",
      "182 activation_57\n",
      "183 average_pooling2d_5\n",
      "184 conv2d_50\n",
      "185 conv2d_53\n",
      "186 conv2d_58\n",
      "187 conv2d_59\n",
      "188 batch_normalization_50\n",
      "189 batch_normalization_53\n",
      "190 batch_normalization_58\n",
      "191 batch_normalization_59\n",
      "192 activation_50\n",
      "193 activation_53\n",
      "194 activation_58\n",
      "195 activation_59\n",
      "196 mixed6\n",
      "197 conv2d_64\n",
      "198 batch_normalization_64\n",
      "199 activation_64\n",
      "200 conv2d_65\n",
      "201 batch_normalization_65\n",
      "202 activation_65\n",
      "203 conv2d_61\n",
      "204 conv2d_66\n",
      "205 batch_normalization_61\n",
      "206 batch_normalization_66\n",
      "207 activation_61\n",
      "208 activation_66\n",
      "209 conv2d_62\n",
      "210 conv2d_67\n",
      "211 batch_normalization_62\n",
      "212 batch_normalization_67\n",
      "213 activation_62\n",
      "214 activation_67\n",
      "215 average_pooling2d_6\n",
      "216 conv2d_60\n",
      "217 conv2d_63\n",
      "218 conv2d_68\n",
      "219 conv2d_69\n",
      "220 batch_normalization_60\n",
      "221 batch_normalization_63\n",
      "222 batch_normalization_68\n",
      "223 batch_normalization_69\n",
      "224 activation_60\n",
      "225 activation_63\n",
      "226 activation_68\n",
      "227 activation_69\n",
      "228 mixed7\n",
      "229 conv2d_72\n",
      "230 batch_normalization_72\n",
      "231 activation_72\n",
      "232 conv2d_73\n",
      "233 batch_normalization_73\n",
      "234 activation_73\n",
      "235 conv2d_70\n",
      "236 conv2d_74\n",
      "237 batch_normalization_70\n",
      "238 batch_normalization_74\n",
      "239 activation_70\n",
      "240 activation_74\n",
      "241 conv2d_71\n",
      "242 conv2d_75\n",
      "243 batch_normalization_71\n",
      "244 batch_normalization_75\n",
      "245 activation_71\n",
      "246 activation_75\n",
      "247 max_pooling2d_3\n",
      "248 mixed8\n",
      "249 conv2d_80\n",
      "250 batch_normalization_80\n",
      "251 activation_80\n",
      "252 conv2d_77\n",
      "253 conv2d_81\n",
      "254 batch_normalization_77\n",
      "255 batch_normalization_81\n",
      "256 activation_77\n",
      "257 activation_81\n",
      "258 conv2d_78\n",
      "259 conv2d_79\n",
      "260 conv2d_82\n",
      "261 conv2d_83\n",
      "262 average_pooling2d_7\n",
      "263 conv2d_76\n",
      "264 batch_normalization_78\n",
      "265 batch_normalization_79\n",
      "266 batch_normalization_82\n",
      "267 batch_normalization_83\n",
      "268 conv2d_84\n",
      "269 batch_normalization_76\n",
      "270 activation_78\n",
      "271 activation_79\n",
      "272 activation_82\n",
      "273 activation_83\n",
      "274 batch_normalization_84\n",
      "275 activation_76\n",
      "276 mixed9_0\n",
      "277 concatenate\n",
      "278 activation_84\n",
      "279 mixed9\n",
      "280 conv2d_89\n",
      "281 batch_normalization_89\n",
      "282 activation_89\n",
      "283 conv2d_86\n",
      "284 conv2d_90\n",
      "285 batch_normalization_86\n",
      "286 batch_normalization_90\n",
      "287 activation_86\n",
      "288 activation_90\n",
      "289 conv2d_87\n",
      "290 conv2d_88\n",
      "291 conv2d_91\n",
      "292 conv2d_92\n",
      "293 average_pooling2d_8\n",
      "294 conv2d_85\n",
      "295 batch_normalization_87\n",
      "296 batch_normalization_88\n",
      "297 batch_normalization_91\n",
      "298 batch_normalization_92\n",
      "299 conv2d_93\n",
      "300 batch_normalization_85\n",
      "301 activation_87\n",
      "302 activation_88\n",
      "303 activation_91\n",
      "304 activation_92\n",
      "305 batch_normalization_93\n",
      "306 activation_85\n",
      "307 mixed9_1\n",
      "308 concatenate_1\n",
      "309 activation_93\n",
      "310 mixed10\n",
      "311 global_average_pooling2d\n",
      "312 dense_layer\n",
      "313 output_fc\n"
     ]
    }
   ],
   "source": [
    "model = build_cnn(config)\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)\n",
    "#model.summary(line_length=150)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCsZqqHyqAds"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-CFjGGnr1iDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102 276\n"
     ]
    }
   ],
   "source": [
    "origin_train_len = len(train_df) / 5 * 4\n",
    "origin_val_len = len(train_df) / 5 * 1\n",
    "\n",
    "train_num_steps = int(np.ceil((origin_train_len)/config['batch_size']))\n",
    "val_num_steps = int(np.ceil((origin_val_len)/config['batch_size']))\n",
    "\n",
    "print(train_num_steps, val_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YzKzI0vXsrJp"
   },
   "outputs": [],
   "source": [
    "model_base_path = data_base_path\n",
    "model_checkpoint_path = pth.join(model_base_path, 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001*10*0.5\n",
    "    lr_max     = 0.0000005 * config['batch_size'] * 10*0.5\n",
    "    lr_min     = 0.000001 * 10*0.5\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "     \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ta_kqsLstQcV",
    "outputId": "760909ad-2f6c-444c-aa85-43ab5e364c6b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 73/10000\n",
      "    882/Unknown - 397s 450ms/step - loss: 0.0127 - acc: 0.9969 - precision: 0.9978 - recall: 0.9964 - auc: 0.9997\n",
      "Epoch 00073: val_loss improved from inf to 0.44805, saving model to data/public/checkpoint/InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O/000073-0.448049-0.012730.hdf5\n",
      "882/882 [==============================] - 490s 555ms/step - loss: 0.0127 - acc: 0.9969 - precision: 0.9978 - recall: 0.9964 - auc: 0.9997 - val_loss: 0.4480 - val_acc: 0.9242 - val_precision: 0.9398 - val_recall: 0.9199 - val_auc: 0.9848\n",
      "Epoch 74/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9975 - precision: 0.9981 - recall: 0.9971 - auc: 0.9998\n",
      "Epoch 00074: val_loss improved from 0.44805 to 0.44063, saving model to data/public/checkpoint/InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O/000074-0.440635-0.010605.hdf5\n",
      "882/882 [==============================] - 489s 554ms/step - loss: 0.0106 - acc: 0.9975 - precision: 0.9981 - recall: 0.9971 - auc: 0.9998 - val_loss: 0.4406 - val_acc: 0.9241 - val_precision: 0.9398 - val_recall: 0.9193 - val_auc: 0.9854\n",
      "Epoch 75/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9972 - precision: 0.9978 - recall: 0.9967 - auc: 0.9997\n",
      "Epoch 00075: val_loss did not improve from 0.44063\n",
      "882/882 [==============================] - 489s 555ms/step - loss: 0.0120 - acc: 0.9972 - precision: 0.9978 - recall: 0.9967 - auc: 0.9997 - val_loss: 0.4572 - val_acc: 0.9240 - val_precision: 0.9404 - val_recall: 0.9194 - val_auc: 0.9847\n",
      "Epoch 76/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9974 - precision: 0.9979 - recall: 0.9970 - auc: 0.9997\n",
      "Epoch 00076: val_loss did not improve from 0.44063\n",
      "882/882 [==============================] - 490s 555ms/step - loss: 0.0114 - acc: 0.9974 - precision: 0.9979 - recall: 0.9970 - auc: 0.9997 - val_loss: 0.4485 - val_acc: 0.9249 - val_precision: 0.9420 - val_recall: 0.9202 - val_auc: 0.9850\n",
      "Epoch 77/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9973 - precision: 0.9981 - recall: 0.9970 - auc: 0.9998\n",
      "Epoch 00077: val_loss improved from 0.44063 to 0.43887, saving model to data/public/checkpoint/InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O/000077-0.438874-0.010855.hdf5\n",
      "882/882 [==============================] - 489s 555ms/step - loss: 0.0109 - acc: 0.9973 - precision: 0.9981 - recall: 0.9970 - auc: 0.9998 - val_loss: 0.4389 - val_acc: 0.9254 - val_precision: 0.9423 - val_recall: 0.9204 - val_auc: 0.9852\n",
      "Epoch 78/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9975 - precision: 0.9982 - recall: 0.9971 - auc: 0.9998\n",
      "Epoch 00078: val_loss did not improve from 0.43887\n",
      "882/882 [==============================] - 489s 555ms/step - loss: 0.0100 - acc: 0.9975 - precision: 0.9982 - recall: 0.9971 - auc: 0.9998 - val_loss: 0.4402 - val_acc: 0.9253 - val_precision: 0.9415 - val_recall: 0.9205 - val_auc: 0.9852\n",
      "Epoch 79/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9976 - precision: 0.9982 - recall: 0.9973 - auc: 0.9998\n",
      "Epoch 00079: val_loss did not improve from 0.43887\n",
      "882/882 [==============================] - 489s 554ms/step - loss: 0.0098 - acc: 0.9976 - precision: 0.9982 - recall: 0.9973 - auc: 0.9998 - val_loss: 0.4435 - val_acc: 0.9268 - val_precision: 0.9435 - val_recall: 0.9219 - val_auc: 0.9851\n",
      "Epoch 80/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9974 - precision: 0.9981 - recall: 0.9970 - auc: 0.9998\n",
      "Epoch 00080: val_loss did not improve from 0.43887\n",
      "882/882 [==============================] - 490s 556ms/step - loss: 0.0102 - acc: 0.9974 - precision: 0.9981 - recall: 0.9970 - auc: 0.9998 - val_loss: 0.4469 - val_acc: 0.9262 - val_precision: 0.9418 - val_recall: 0.9220 - val_auc: 0.9851\n",
      "Epoch 81/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9975 - precision: 0.9982 - recall: 0.9971 - auc: 0.9998\n",
      "Epoch 00081: val_loss did not improve from 0.43887\n",
      "882/882 [==============================] - 490s 555ms/step - loss: 0.0101 - acc: 0.9975 - precision: 0.9982 - recall: 0.9971 - auc: 0.9998 - val_loss: 0.4452 - val_acc: 0.9258 - val_precision: 0.9417 - val_recall: 0.9207 - val_auc: 0.9847\n",
      "Epoch 82/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9972 - precision: 0.9982 - recall: 0.9969 - auc: 0.9998\n",
      "Epoch 00082: val_loss did not improve from 0.43887\n",
      "882/882 [==============================] - 490s 556ms/step - loss: 0.0108 - acc: 0.9972 - precision: 0.9982 - recall: 0.9969 - auc: 0.9998 - val_loss: 0.4445 - val_acc: 0.9264 - val_precision: 0.9415 - val_recall: 0.9215 - val_auc: 0.9848\n",
      "Epoch 83/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9976 - precision: 0.9982 - recall: 0.9972 - auc: 0.9998\n",
      "Epoch 00083: val_loss improved from 0.43887 to 0.43588, saving model to data/public/checkpoint/InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O/000083-0.435883-0.010438.hdf5\n",
      "882/882 [==============================] - 490s 555ms/step - loss: 0.0104 - acc: 0.9976 - precision: 0.9982 - recall: 0.9972 - auc: 0.9998 - val_loss: 0.4359 - val_acc: 0.9267 - val_precision: 0.9426 - val_recall: 0.9222 - val_auc: 0.9857\n",
      "Epoch 84/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9974 - precision: 0.9982 - recall: 0.9970 - auc: 0.9998\n",
      "Epoch 00084: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 489s 555ms/step - loss: 0.0107 - acc: 0.9974 - precision: 0.9982 - recall: 0.9970 - auc: 0.9998 - val_loss: 0.4473 - val_acc: 0.9248 - val_precision: 0.9414 - val_recall: 0.9198 - val_auc: 0.9854\n",
      "Epoch 85/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9973 - precision: 0.9980 - recall: 0.9969 - auc: 0.9997\n",
      "Epoch 00085: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 490s 556ms/step - loss: 0.0120 - acc: 0.9973 - precision: 0.9980 - recall: 0.9969 - auc: 0.9997 - val_loss: 0.4550 - val_acc: 0.9240 - val_precision: 0.9397 - val_recall: 0.9188 - val_auc: 0.9846\n",
      "Epoch 86/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9973 - precision: 0.9980 - recall: 0.9969 - auc: 0.9998\n",
      "Epoch 00086: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 491s 557ms/step - loss: 0.0103 - acc: 0.9973 - precision: 0.9980 - recall: 0.9969 - auc: 0.9998 - val_loss: 0.4458 - val_acc: 0.9246 - val_precision: 0.9418 - val_recall: 0.9200 - val_auc: 0.9851\n",
      "Epoch 87/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9976 - precision: 0.9981 - recall: 0.9972 - auc: 0.9998\n",
      "Epoch 00087: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 491s 557ms/step - loss: 0.0108 - acc: 0.9976 - precision: 0.9981 - recall: 0.9972 - auc: 0.9998 - val_loss: 0.4510 - val_acc: 0.9236 - val_precision: 0.9400 - val_recall: 0.9190 - val_auc: 0.9852\n",
      "Epoch 88/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9977 - precision: 0.9985 - recall: 0.9974 - auc: 0.9998\n",
      "Epoch 00088: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 490s 555ms/step - loss: 0.0094 - acc: 0.9977 - precision: 0.9985 - recall: 0.9974 - auc: 0.9998 - val_loss: 0.4523 - val_acc: 0.9243 - val_precision: 0.9410 - val_recall: 0.9191 - val_auc: 0.9855\n",
      "Epoch 89/10000\n",
      "881/882 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9974 - precision: 0.9982 - recall: 0.9970 - auc: 0.9998\n",
      "Epoch 00089: val_loss did not improve from 0.43588\n",
      "882/882 [==============================] - 490s 556ms/step - loss: 0.0106 - acc: 0.9974 - precision: 0.9982 - recall: 0.9970 - auc: 0.9998 - val_loss: 0.4474 - val_acc: 0.9255 - val_precision: 0.9416 - val_recall: 0.9199 - val_auc: 0.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "521/882 [================>.............] - ETA: 2:42 - loss: 0.0119 - acc: 0.9972 - precision: 0.9978 - recall: 0.9968 - auc: 0.9997"
     ]
    }
   ],
   "source": [
    "for conv_comb, activation, base_channel, \\\n",
    "    between_type, fc_num, batch_size \\\n",
    "        in itertools.product(conv_comb_list, activation_list,\n",
    "                              base_channel_list, between_type_list, fc_list,\n",
    "                              batch_size_list):\n",
    "    config['conv']['conv_num'] = conv_comb\n",
    "    config['conv']['base_channel'] = base_channel\n",
    "    config['activation'] = activation\n",
    "    config['between_type'] = between_type\n",
    "    config['fc']['fc_num'] = fc_num\n",
    "    config['batch_size'] = batch_size\n",
    "\n",
    "    base = BASE_MODEL_NAME\n",
    "\n",
    "    base += '_resize_{}'.format(config['aug']['resize'][0])\n",
    "\n",
    "    base += '_conv_{}'.format('-'.join(map(lambda x:str(x),config['conv']['conv_num'])))\n",
    "    base += '_basech_{}'.format(config['conv']['base_channel'])\n",
    "    base += '_act_{}'.format(config['activation'])\n",
    "    base += '_pool_{}'.format(config['pool']['type'])\n",
    "    base += '_betw_{}'.format(config['between_type'])\n",
    "    base += '_fc_{}'.format(config['fc']['fc_num'])\n",
    "    base += '_zscore_{}'.format(config['is_zscore'])\n",
    "    base += '_batch_{}'.format(config['batch_size'])\n",
    "    if config['is_dropout']:\n",
    "        base += '_DO_'+str(config['dropout_rate']).replace('.', '')\n",
    "    if config['is_batchnorm']:\n",
    "        base += '_BN'+'_O'\n",
    "    else:\n",
    "        base += '_BN'+'_X'\n",
    "\n",
    "    model_name = base\n",
    "    print(model_name)\n",
    "\n",
    "    ### Define dataset\n",
    "    dataset = tf.data.TFRecordDataset(train_tfrecord_path, compression_type='GZIP')\n",
    "    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # dataset = dataset.cache()\n",
    "    dataset = dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(resize_and_crop_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(image_aug_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(config['buffer_size'])\n",
    "    dataset = dataset.batch(config['batch_size'])\n",
    "    dataset = dataset.map(post_process_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.TFRecordDataset(val_tfrecord_path, compression_type='GZIP')\n",
    "    val_dataset = val_dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = val_dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = val_dataset.map(resize_and_crop_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # val_dataset = val_dataset.map(image_aug_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # val_dataset = val_dataset.shuffle(config['buffer_size'])\n",
    "    val_dataset = val_dataset.batch(config['batch_size'])\n",
    "    val_dataset = val_dataset.map(post_process_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # val_dataset = val_dataset.cache()\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model_path = pth.join(\n",
    "        model_checkpoint_path, model_name, \n",
    "    )\n",
    "    model = build_cnn(config)\n",
    "    #         model.summary()\n",
    "    \n",
    "    initial_epoch = 0\n",
    "\n",
    "    if pth.isdir(model_path) and len([_ for _ in os.listdir(model_path) if _.endswith('hdf5')]) >= 1:\n",
    "        model.compile(loss=config['loss'], optimizer=Adam(lr=config['learning_rate']),\n",
    "                  metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
    "        \n",
    "        model_chk_name = sorted(os.listdir(model_path))[-1]\n",
    "        initial_epoch = int(model_chk_name.split('-')[0])\n",
    "        model.load_weights(pth.join(model_path, model_chk_name))\n",
    "    else:\n",
    "        # first: train only the top layers (which were randomly initialized)\n",
    "        # i.e. freeze all convolutional InceptionV3 layers\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                     metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
    "        \n",
    "        PRE_TRAIN_EPOCH = 6\n",
    "        model.fit(\n",
    "            x=dataset, epochs=PRE_TRAIN_EPOCH, # train only top layers for just a few epochs.\n",
    "            validation_data=val_dataset, shuffle=True,\n",
    "            #callbacks = [checkpointer, es], #batch_size=config['batch_size']\n",
    "            initial_epoch=initial_epoch,\n",
    "            # steps_per_epoch=train_num_steps, validation_steps=val_num_steps,\n",
    "            verbose=1)\n",
    "        \n",
    "        # at this point, the top layers are well trained and we can start fine-tuning\n",
    "        # convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "        # and train the remaining top layers.\n",
    "        \n",
    "        # let's visualize layer names and layer indices to see how many layers\n",
    "        # we should freeze:\n",
    "        for i, layer in enumerate(model.layers):\n",
    "           print(i, layer.name)\n",
    "        \n",
    "        # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "        # the first 249 layers and unfreeze the rest:\n",
    "        for layer in model.layers[:229]:  # [:249]:\n",
    "           layer.trainable = False\n",
    "        for layer in model.layers[229:]:  # [249:]:\n",
    "           layer.trainable = True\n",
    "        \n",
    "        # we need to recompile the model for these modifications to take effect\n",
    "        # we use Adam with a low learning rate\n",
    "        model.compile(loss=config['loss'], optimizer=Adam(lr=config['learning_rate']),\n",
    "            metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
    "        \n",
    "        initial_epoch = PRE_TRAIN_EPOCH\n",
    "\n",
    "        \n",
    "    # IGNORE 4 lines below in InceptionV3 \n",
    "    # ### Freeze first layer\n",
    "    # conv_list = [layer for layer in model.layers if isinstance(layer, keras.layers.Conv2D)]\n",
    "    # conv_list[0].trainable = False\n",
    "    # # conv_list[1].trainable = False\n",
    "\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = pth.join(model_path, '{epoch:06d}-{val_loss:0.6f}-{loss:0.6f}.hdf5')\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=model_filename, verbose=1, \n",
    "        period=1, save_best_only=True, \n",
    "        monitor='val_loss'\n",
    "    )\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=16)  ### 16 at night. 10 genral, 6 for experiment\n",
    "\n",
    "    hist = model.fit(\n",
    "        x=dataset, epochs=config['num_epoch'], \n",
    "        validation_data=val_dataset, shuffle=True,\n",
    "        callbacks = [get_lr_callback(), checkpointer, es], #batch_size=config['batch_size']\n",
    "        initial_epoch=initial_epoch,\n",
    "        # steps_per_epoch=train_num_steps, validation_steps=val_num_steps,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_analysis_path = model_path.replace('checkpoint', 'analysis')\n",
    "    visualization_path = pth.join(model_analysis_path,'visualization')\n",
    "    os.makedirs(visualization_path, exist_ok=True)\n",
    "    \n",
    "    print()\n",
    "    # clear_output()        \n",
    "    for each_label in ['loss', 'acc', 'precision', 'recall', 'auc']:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(hist.history[each_label], 'g', label='train_{}'.format(each_label))\n",
    "        ax.plot(hist.history['val_{}'.format(each_label)], 'r', label='val_{}'.format(each_label))\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.legend(loc='upper left')\n",
    "        if not each_label == 'loss':\n",
    "            plt.ylim(0, 1)\n",
    "        plt.show()\n",
    "        filename = 'learning_curve_{}'.format(each_label)\n",
    "#             fig.savefig(pth.join(visualization_path, filename), transparent=True)\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "    np.savez_compressed(pth.join(visualization_path, 'learning_curve'), \n",
    "                        loss=hist.history['loss'], \n",
    "                        val_loss=hist.history['val_loss'],\n",
    "                        acc=hist.history['acc'], \n",
    "                        val_acc=hist.history['val_acc'],\n",
    "                        precision=hist.history['precision'], \n",
    "                        vaval_precisionl_mae=hist.history['val_precision'],  \n",
    "                        recall=hist.history['recall'],\n",
    "                        val_recall=hist.history['val_recall'],\n",
    "                        auc=hist.history['auc'],\n",
    "                        val_auc=hist.history['val_auc']\n",
    "                        )\n",
    "\n",
    "    model.save(pth.join(model_path, '000000_last.hdf5'))\n",
    "    K.clear_session()\n",
    "    del(model)\n",
    "    \n",
    "    model_analysis_base_path = pth.join(model_base_path, 'analysis', model_name) \n",
    "    with open(pth.join(model_analysis_base_path, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    chk_name_list = sorted([name for name in os.listdir(model_path) if name != '000000_last.hdf5'])\n",
    "    for chk_name in chk_name_list[:-2]:\n",
    "        os.remove(pth.join(model_path, chk_name))\n",
    "    # clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibXfENT5zvwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57ARllmjWGk-"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u1S7DrvwhFPM"
   },
   "outputs": [],
   "source": [
    "image_feature_description_for_test = {\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    # 'randmark_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    # 'id': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function_for_test(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description_for_test)\n",
    "\n",
    "def map_func_for_test(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def resize_and_crop_func_for_test(image):\n",
    "    result_image = tf.image.resize(image, config['aug']['resize'])\n",
    "    result_image = tf.image.random_crop(image, size=config['input_shape'], seed=7777)\n",
    "    return result_image\n",
    "\n",
    "def post_process_func_for_test(image):\n",
    "    # result_image = result_image / 255\n",
    "    result_image = my_model_base.preprocess_input(image)\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wi2igBp6WSYD"
   },
   "outputs": [],
   "source": [
    "submission_base_path = pth.join(data_base_path, 'submission')\n",
    "os.makedirs(submission_base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "71z9_wKaMPTJ",
    "outputId": "aae36664-100b-46f1-ebd1-25a3804e4b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3-for-upload_resize_297_conv_0_basech_0_act_relu_pool_X_betw_avg_fc_0_zscore_True_batch_80_BN_O\n",
      "      1/Unknown - 0s 51us/stepWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_predict_batch_end` time: 0.3535s). Check your callbacks.\n",
      "475/475 [==============================] - 185s 389ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for conv_comb, activation, base_channel, \\\n",
    "    between_type, fc_num, batch_size \\\n",
    "        in itertools.product(conv_comb_list, activation_list,\n",
    "                              base_channel_list, between_type_list, fc_list,\n",
    "                              batch_size_list):\n",
    "    config['conv']['conv_num'] = conv_comb\n",
    "    config['conv']['base_channel'] = base_channel\n",
    "    config['activation'] = activation\n",
    "    config['between_type'] = between_type\n",
    "    config['fc']['fc_num'] = fc_num\n",
    "    config['batch_size'] = batch_size\n",
    "\n",
    "    base = BASE_MODEL_NAME\n",
    "\n",
    "    base += '_resize_{}'.format(config['aug']['resize'][0])\n",
    "\n",
    "    base += '_conv_{}'.format('-'.join(map(lambda x:str(x),config['conv']['conv_num'])))\n",
    "    base += '_basech_{}'.format(config['conv']['base_channel'])\n",
    "    base += '_act_{}'.format(config['activation'])\n",
    "    base += '_pool_{}'.format(config['pool']['type'])\n",
    "    base += '_betw_{}'.format(config['between_type'])\n",
    "    base += '_fc_{}'.format(config['fc']['fc_num'])\n",
    "    base += '_zscore_{}'.format(config['is_zscore'])\n",
    "    base += '_batch_{}'.format(config['batch_size'])\n",
    "    if config['is_dropout']:\n",
    "        base += '_DO_'+str(config['dropout_rate']).replace('.', '')\n",
    "    if config['is_batchnorm']:\n",
    "        base += '_BN'+'_O'\n",
    "    else:\n",
    "        base += '_BN'+'_X'\n",
    "\n",
    "    model_name = base\n",
    "    print(model_name)\n",
    "\n",
    "    ### Define dataset\n",
    "    test_dataset = tf.data.TFRecordDataset(test_tfrecord_path, compression_type='GZIP')\n",
    "    test_dataset = test_dataset.map(_parse_image_function_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.map(map_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.map(resize_and_crop_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(config['batch_size'])\n",
    "    test_dataset = test_dataset.map(post_process_func_for_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model_path = pth.join(\n",
    "        model_checkpoint_path, model_name, \n",
    "    )\n",
    "    model = build_cnn(config)\n",
    "    #         model.summary()\n",
    "    model.compile(loss=config['loss'], optimizer=Adam(lr=config['learning_rate']),\n",
    "                  metrics=['acc', 'Precision', 'Recall', 'AUC'])\n",
    "    initial_epoch = 0\n",
    "\n",
    "    model_chk_name = sorted(os.listdir(model_path))[-1]\n",
    "    initial_epoch = int(model_chk_name.split('-')[0])\n",
    "    model.load_weights(pth.join(model_path, model_chk_name))\n",
    "\n",
    "    preds = model.predict(test_dataset, verbose=1)\n",
    "    \n",
    "    #pred_labels = np.argmax(preds, axis=1)\n",
    "    #pred_probs = np.array([pred[indice] for pred, indice in zip(preds, pred_labels)])\n",
    "    \n",
    "    # argmax --> top3\n",
    "    pred_labels = np.argsort(-preds)\n",
    "    \n",
    "    submission_csv_path = pth.join(data_base_path, submission_csv_name)\n",
    "    submission_df = pd.read_csv(submission_csv_path)\n",
    "    \n",
    "    merged_df = []\n",
    "    \n",
    "    RANK_TO_SAVE = 5\n",
    "    for i in range(RANK_TO_SAVE):\n",
    "        tmp_df = submission_df.copy()\n",
    "        \n",
    "        tmp_labels = pred_labels[:, i]\n",
    "        tmp_df['landmark_id'] = tmp_labels\n",
    "        tmp_df['conf'] = np.array([pred[indice] for pred, indice in zip(preds, tmp_labels)])\n",
    "        merged_df.append(tmp_df)\n",
    "    \n",
    "    submission_df = pd.concat(merged_df)\n",
    "    \n",
    "    #submission_df['landmark_id'] = pred_labels\n",
    "    #submission_df['conf'] = pred_probs\n",
    "\n",
    "    today_str = datetime.date.today().strftime('%Y%m%d')\n",
    "    result_filename = '{}.csv'.format(model_name)\n",
    "    submission_csv_fileaname = pth.join(submission_base_path, '_'.join([today_str, result_filename]))\n",
    "    submission_df.to_csv(submission_csv_fileaname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMStwUj7nYz9"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Training_MobileNetV2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
